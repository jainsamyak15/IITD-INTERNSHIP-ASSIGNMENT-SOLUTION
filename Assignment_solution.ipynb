{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1eeAHgu-fzp28PGdIjeLe-pzGPMG2r2G_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRLAiUDyIjhj",
        "outputId": "d080108a-9be8-44a3-d27b-5fa96806a8a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1eeAHgu-fzp28PGdIjeLe-pzGPMG2r2G_\n",
            "From (redirected): https://drive.google.com/uc?id=1eeAHgu-fzp28PGdIjeLe-pzGPMG2r2G_&confirm=t&uuid=73d4e9fe-d106-4c23-8bda-f5f20af5b3ce\n",
            "To: /content/DINO/checkpoint0011_4scale.pth\n",
            "100% 562M/562M [00:18<00:00, 30.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yapf==0.31.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U44gkjZJJKP3",
        "outputId": "de1cd417-6ea7-443a-cf64-42e7eb997bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yapf==0.31.0\n",
            "  Downloading yapf-0.31.0-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Downloading yapf-0.31.0-py2.py3-none-any.whl (185 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/185.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.7/185.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yapf\n",
            "  Attempting uninstall: yapf\n",
            "    Found existing installation: yapf 0.40.2\n",
            "    Uninstalling yapf-0.40.2:\n",
            "      Successfully uninstalled yapf-0.40.2\n",
            "Successfully installed yapf-0.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "base_path = '/content/drive/MyDrive/iit_delhi_pedestrian_dataset'\n",
        "coco_dir = '/content/COCODIR'\n",
        "\n",
        "# Create COCO directory structure\n",
        "os.makedirs(os.path.join(coco_dir, 'train2017'), exist_ok=True)\n",
        "os.makedirs(os.path.join(coco_dir, 'val2017'), exist_ok=True)\n",
        "os.makedirs(os.path.join(coco_dir, 'annotations'), exist_ok=True)\n",
        "\n",
        "# Copy images\n",
        "for split in ['train', 'val']:\n",
        "    src_dir = os.path.join(base_path, split)\n",
        "    dst_dir = os.path.join(coco_dir, f'{split}2017')\n",
        "    for img in os.listdir(src_dir):\n",
        "        shutil.copy(os.path.join(src_dir, img), os.path.join(dst_dir, img))\n",
        "\n",
        "# Process annotations\n",
        "for split in ['train', 'val']:\n",
        "    with open(os.path.join(base_path, f'{split}_annotations.json'), 'r') as f:\n",
        "        annotations = json.load(f)\n",
        "\n",
        "    # Update image filenames\n",
        "    for img in annotations['images']:\n",
        "        img['file_name'] = img['file_name'].replace(f'{split}/', f'{split}2017/')\n",
        "\n",
        "    # Save in COCO format\n",
        "    with open(os.path.join(coco_dir, 'annotations', f'instances_{split}2017.json'), 'w') as f:\n",
        "        json.dump(annotations, f)\n",
        "\n",
        "print(\"Dataset preparation completed.\")\n",
        "\n",
        "# Clone DINO repository\n",
        "!git clone https://github.com/IDEA-Research/DINO.git\n",
        "%cd DINO\n",
        "\n",
        "# Install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "%cd models/dino/ops\n",
        "!python setup.py build install\n",
        "%cd ../../..\n",
        "\n",
        "# # Download pre-trained model\n",
        "# !wget https://github.com/IDEA-Research/DINO/releases/download/v0.1.0/checkpoint0011_4scale.pth\n",
        "\n",
        "# Run evaluation\n",
        "!bash scripts/DINO_eval.sh {coco_dir} checkpoint0011_4scale.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SL9k7LKMN6h",
        "outputId": "33cdc0a1-29cf-44f3-8965-a46a21041805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset preparation completed.\n",
            "Cloning into 'DINO'...\n",
            "remote: Enumerating objects: 442, done.\u001b[K\n",
            "remote: Counting objects: 100% (191/191), done.\u001b[K\n",
            "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
            "remote: Total 442 (delta 136), reused 96 (delta 96), pack-reused 251 (from 1)\u001b[K\n",
            "Receiving objects: 100% (442/442), 13.43 MiB | 21.90 MiB/s, done.\n",
            "Resolving deltas: 100% (191/191), done.\n",
            "/content/DINO/DINO\n",
            "Collecting pycocotools (from -r requirements.txt (line 2))\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-install-xbr0i5iw/pycocotools_deda5ef90a7641c496e73dd0c4742fd4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-install-xbr0i5iw/pycocotools_deda5ef90a7641c496e73dd0c4742fd4\n",
            "  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting panopticapi (from -r requirements.txt (line 6))\n",
            "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-install-xbr0i5iw/panopticapi_1954cd6d72cc47fbaf498e9973717f09\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/panopticapi.git /tmp/pip-install-xbr0i5iw/panopticapi_1954cd6d72cc47fbaf498e9973717f09\n",
            "  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.0.11)\n",
            "Requirement already satisfied: submitit in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.5.2)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.19.1+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.13.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.4.0)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.4.0)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.31.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (1.0.9)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools->-r requirements.txt (line 2)) (71.0.4)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools->-r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 3)) (2.2.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 5)) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 5)) (10.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 11)) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 11)) (0.4.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 11)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 11)) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->-r requirements.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.0->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (2024.8.30)\n",
            "/content/DINO/DINO/models/dino/ops\n",
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-cpython-310\n",
            "creating build/lib.linux-x86_64-cpython-310/functions\n",
            "copying functions/__init__.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "copying functions/ms_deform_attn_func.py -> build/lib.linux-x86_64-cpython-310/functions\n",
            "creating build/lib.linux-x86_64-cpython-310/modules\n",
            "copying modules/ms_deform_attn.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "copying modules/__init__.py -> build/lib.linux-x86_64-cpython-310/modules\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:495: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "building 'MultiScaleDeformableAttention' extension\n",
            "creating build/temp.linux-x86_64-cpython-310\n",
            "creating build/temp.linux-x86_64-cpython-310/content\n",
            "creating build/temp.linux-x86_64-cpython-310/content/DINO\n",
            "creating build/temp.linux-x86_64-cpython-310/content/DINO/DINO\n",
            "creating build/temp.linux-x86_64-cpython-310/content/DINO/DINO/models\n",
            "creating build/temp.linux-x86_64-cpython-310/content/DINO/DINO/models/dino\n",
            "creating build/temp.linux-x86_64-cpython-310/content/DINO/DINO/models/dino/ops\n",
            "creating build/temp.linux-x86_64-cpython-310/content/DINO/DINO/models/dino/ops/src\n",
            "creating build/temp.linux-x86_64-cpython-310/content/DINO/DINO/models/dino/ops/src/cpu\n",
            "creating build/temp.linux-x86_64-cpython-310/content/DINO/DINO/models/dino/ops/src/cuda\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DWITH_CUDA -I/content/DINO/DINO/models/dino/ops/src -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c /content/DINO/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.cpp -o build/temp.linux-x86_64-cpython-310/content/DINO/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/cuda/bin/nvcc -DWITH_CUDA -I/content/DINO/DINO/models/dino/ops/src -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c /content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu -o build/temp.linux-x86_64-cpython-310/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(261)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_im2col_cuda(cudaStream_t, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 64 of /content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(762)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 134 of /content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(872)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 134 of /content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(331)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 134 of /content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(436)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 134 of /content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(544)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 134 of /content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(649)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"q_col\"\u001b[0m was declared but never referenced\n",
            "      const int q_col = _temp % num_query;\n",
            "                ^\n",
            "          detected during instantiation of \u001b[01m\"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\"\u001b[0m \u001b[32mat line 134 of /content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor ms_deform_attn_cuda_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:34:61:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   34 |     AT_ASSERTM(value.type().is_cuda(), \"value must\u001b[01;35m\u001b[K be a CUDA t\u001b[m\u001b[Kensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:35:70:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   35 |     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"s\u001b[01;35m\u001b[Kpatial_shapes must be\u001b[m\u001b[K a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:36:73:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   36 |     AT_ASSERTM(level_start_index.type().is_cuda(),\u001b[01;35m\u001b[K \"level_start_index must\u001b[m\u001b[K be a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:37:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   37 |     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sam\u001b[01;35m\u001b[Kpling_loc must be a\u001b[m\u001b[K CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:38:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   38 |     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn\u001b[01;35m\u001b[K_weight must be a \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_\u001b[01;35m\u001b[KTYPES(value.ty\u001b[m\u001b[Kpe(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K         \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:109:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1045:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1131:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1174:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1207:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1290:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1448:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2310:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2396:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2439:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2471:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2553:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2710:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   64 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> ms_deform_attn_cuda_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:100:61:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  100 |     AT_ASSERTM(value.type().is_cuda(), \"value must\u001b[01;35m\u001b[K be a CUDA t\u001b[m\u001b[Kensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:101:70:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  101 |     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"s\u001b[01;35m\u001b[Kpatial_shapes must be\u001b[m\u001b[K a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:102:73:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  102 |     AT_ASSERTM(level_start_index.type().is_cuda(),\u001b[01;35m\u001b[K \"level_start_index must\u001b[m\u001b[K be a CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:103:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  103 |     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sam\u001b[01;35m\u001b[Kpling_loc must be a\u001b[m\u001b[K CUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:104:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  104 |     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn\u001b[01;35m\u001b[K_weight must be a \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:105:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  105 |     AT_ASSERTM(grad_output.type().is_cuda(), \"grad\u001b[01;35m\u001b[K_output must be a \u001b[m\u001b[KCUDA tensor\");\n",
            "      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_\u001b[01;35m\u001b[KTYPES(value.ty\u001b[m\u001b[Kpe(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:164:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K         \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/Dispatch.h:109:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1055:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1081:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1167:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1210:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1243:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1326:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1487:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1571:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1659:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2582:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2607:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2693:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2736:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = long int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2768:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2850:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:3010:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:3093:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:3180:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  134 |         AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n",
            "      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K \n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:247:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  247 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n",
            "      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DWITH_CUDA -I/content/DINO/DINO/models/dino/ops/src -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c /content/DINO/DINO/models/dino/ops/src/vision.cpp -o build/temp.linux-x86_64-cpython-310/content/DINO/DINO/models/dino/ops/src/vision.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "In file included from \u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/ms_deform_attn.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kat::Tensor ms_deform_attn_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/ms_deform_attn.h:29:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   29 |     if (\u001b[01;35m\u001b[Kvalue.type()\u001b[m\u001b[K.is_cuda())\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/ms_deform_attn.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/ms_deform_attn.h:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> ms_deform_attn_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/ms_deform_attn.h:51:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdeprecated-declarations\u0007-Wdeprecated-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   51 |     if (\u001b[01;35m\u001b[Kvalue.type()\u001b[m\u001b[K.is_cuda())\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~^~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/variable.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/torch/extension.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/ms_deform_attn.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/DINO/DINO/models/dino/ops/src/vision.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/torch/include/ATen/core/TensorBody.h:225:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  225 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/content/DINO/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.o build/temp.linux-x86_64-cpython-310/content/DINO/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.o build/temp.linux-x86_64-cpython-310/content/DINO/DINO/models/dino/ops/src/vision.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating MultiScaleDeformableAttention.egg-info\n",
            "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
            "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
            "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/__init__.py -> build/bdist.linux-x86_64/egg/functions\n",
            "copying build/lib.linux-x86_64-cpython-310/functions/ms_deform_attn_func.py -> build/bdist.linux-x86_64/egg/functions\n",
            "creating build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/ms_deform_attn.py -> build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/modules/__init__.py -> build/bdist.linux-x86_64/egg/modules\n",
            "copying build/lib.linux-x86_64-cpython-310/MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/functions/ms_deform_attn_func.py to ms_deform_attn_func.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/ms_deform_attn.py to ms_deform_attn.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/modules/__init__.py to __init__.cpython-310.pyc\n",
            "creating stub loader for MultiScaleDeformableAttention.cpython-310-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-310: module references __file__\n",
            "creating dist\n",
            "creating 'dist/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Extracting MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/MultiScaleDeformableAttention-1.0-py3.10-linux-x86_64.egg\n",
            "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n",
            "/content/DINO/DINO\n",
            "Not using distributed mode\n",
            "Loading config file from config/DINO/DINO_4scale.py\n",
            "[09/24 08:44:20.239]: git:\n",
            "  sha: d84a491d41898b3befd8294d1cf2614661fc0953, status: clean, branch: main\n",
            "\n",
            "[09/24 08:44:20.239]: Command: main.py --output_dir logs/DINO/R50-MS4-%j -c config/DINO/DINO_4scale.py --coco_path /content/COCODIR --eval --resume checkpoint0011_4scale.pth --options dn_scalar=100 embed_init_tgt=TRUE dn_label_coef=1.0 dn_bbox_coef=1.0 use_ema=False dn_box_noise_scale=1.0\n",
            "[09/24 08:44:20.240]: Full config saved to logs/DINO/R50-MS4-%j/config_args_all.json\n",
            "[09/24 08:44:20.240]: world size: 1\n",
            "[09/24 08:44:20.240]: rank: 0\n",
            "[09/24 08:44:20.240]: local_rank: 0\n",
            "[09/24 08:44:20.240]: args: Namespace(config_file='config/DINO/DINO_4scale.py', options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'use_ema': False, 'dn_box_noise_scale': 1.0}, dataset_file='coco', coco_path='/content/COCODIR', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/DINO/R50-MS4-%j', note='', device='cuda', seed=42, resume='checkpoint0011_4scale.pth', pretrain_model_path=None, finetune_ignore=None, start_epoch=0, eval=True, num_workers=10, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=0, amp=False, distributed=False, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_max_size=1333, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, num_classes=91, lr=0.0001, param_dict_type='default', lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, ddetr_lr_param=False, batch_size=2, weight_decay=0.0001, epochs=12, lr_drop=11, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[33, 45], modelname='dino', frozen_weights=None, backbone='resnet50', use_checkpoint=False, dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, unic_layers=0, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, num_feature_levels=4, enc_n_points=4, dec_n_points=4, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_type='standard', two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, transformer_activation='relu', batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=2.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=1.0, mask_loss_coef=1.0, dice_loss_coef=1.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, focal_alpha=0.25, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, use_dn=True, dn_number=100, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, embed_init_tgt=True, dn_labelbook_size=91, match_unstable_error=True, use_ema=False, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, dn_scalar=100, dn_label_coef=1.0, dn_bbox_coef=1.0)\n",
            "\n",
            "Namespace(config_file='config/DINO/DINO_4scale.py', options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'use_ema': False, 'dn_box_noise_scale': 1.0}, dataset_file='coco', coco_path='/content/COCODIR', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/DINO/R50-MS4-%j', note='', device='cuda', seed=42, resume='checkpoint0011_4scale.pth', pretrain_model_path=None, finetune_ignore=None, start_epoch=0, eval=True, num_workers=10, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=0, amp=False, distributed=False, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_max_size=1333, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, num_classes=91, lr=0.0001, param_dict_type='default', lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, ddetr_lr_param=False, batch_size=2, weight_decay=0.0001, epochs=12, lr_drop=11, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[33, 45], modelname='dino', frozen_weights=None, backbone='resnet50', use_checkpoint=False, dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, unic_layers=0, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, num_feature_levels=4, enc_n_points=4, dec_n_points=4, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_type='standard', two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, transformer_activation='relu', batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=2.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=1.0, mask_loss_coef=1.0, dice_loss_coef=1.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, focal_alpha=0.25, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, use_dn=True, dn_number=100, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, embed_init_tgt=True, dn_labelbook_size=91, match_unstable_error=True, use_ema=False, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, dn_scalar=100, dn_label_coef=1.0, dn_bbox_coef=1.0)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "[09/24 08:44:21.834]: number of params:46670782\n",
            "[09/24 08:44:21.835]: params:\n",
            "{\n",
            "  \"transformer.level_embed\": 1024,\n",
            "  \"transformer.encoder.layers.0.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.0.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.0.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.0.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.0.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.0.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.0.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.0.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.0.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.0.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.0.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.1.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.1.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.1.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.1.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.1.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.1.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.1.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.1.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.1.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.1.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.2.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.2.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.2.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.2.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.2.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.2.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.2.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.2.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.2.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.2.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.3.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.3.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.3.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.3.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.3.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.3.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.3.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.3.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.3.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.3.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.4.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.4.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.4.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.4.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.4.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.4.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.4.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.4.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.4.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.4.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.5.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.5.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.5.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.5.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.5.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.5.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.5.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.5.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.5.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.5.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.0.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.0.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.0.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.0.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.0.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.0.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.0.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.0.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.0.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.0.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.1.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.1.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.1.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.1.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.1.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.1.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.1.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.1.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.1.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.1.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.2.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.2.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.2.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.2.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.2.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.2.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.2.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.2.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.2.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.2.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.3.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.3.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.3.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.3.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.3.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.3.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.3.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.3.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.3.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.3.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.4.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.4.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.4.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.4.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.4.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.4.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.4.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.4.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.4.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.4.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.5.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.5.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.5.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.5.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.5.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.5.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.5.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.5.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.5.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.5.norm3.bias\": 256,\n",
            "  \"transformer.decoder.norm.weight\": 256,\n",
            "  \"transformer.decoder.norm.bias\": 256,\n",
            "  \"transformer.decoder.ref_point_head.layers.0.weight\": 131072,\n",
            "  \"transformer.decoder.ref_point_head.layers.0.bias\": 256,\n",
            "  \"transformer.decoder.ref_point_head.layers.1.weight\": 65536,\n",
            "  \"transformer.decoder.ref_point_head.layers.1.bias\": 256,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.0.weight\": 65536,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.0.bias\": 256,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.1.weight\": 65536,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.1.bias\": 256,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.2.weight\": 1024,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.2.bias\": 4,\n",
            "  \"transformer.decoder.class_embed.0.weight\": 23296,\n",
            "  \"transformer.decoder.class_embed.0.bias\": 91,\n",
            "  \"transformer.tgt_embed.weight\": 230400,\n",
            "  \"transformer.enc_output.weight\": 65536,\n",
            "  \"transformer.enc_output.bias\": 256,\n",
            "  \"transformer.enc_output_norm.weight\": 256,\n",
            "  \"transformer.enc_output_norm.bias\": 256,\n",
            "  \"transformer.enc_out_bbox_embed.layers.0.weight\": 65536,\n",
            "  \"transformer.enc_out_bbox_embed.layers.0.bias\": 256,\n",
            "  \"transformer.enc_out_bbox_embed.layers.1.weight\": 65536,\n",
            "  \"transformer.enc_out_bbox_embed.layers.1.bias\": 256,\n",
            "  \"transformer.enc_out_bbox_embed.layers.2.weight\": 1024,\n",
            "  \"transformer.enc_out_bbox_embed.layers.2.bias\": 4,\n",
            "  \"transformer.enc_out_class_embed.weight\": 23296,\n",
            "  \"transformer.enc_out_class_embed.bias\": 91,\n",
            "  \"label_enc.weight\": 23552,\n",
            "  \"input_proj.0.0.weight\": 131072,\n",
            "  \"input_proj.0.0.bias\": 256,\n",
            "  \"input_proj.0.1.weight\": 256,\n",
            "  \"input_proj.0.1.bias\": 256,\n",
            "  \"input_proj.1.0.weight\": 262144,\n",
            "  \"input_proj.1.0.bias\": 256,\n",
            "  \"input_proj.1.1.weight\": 256,\n",
            "  \"input_proj.1.1.bias\": 256,\n",
            "  \"input_proj.2.0.weight\": 524288,\n",
            "  \"input_proj.2.0.bias\": 256,\n",
            "  \"input_proj.2.1.weight\": 256,\n",
            "  \"input_proj.2.1.bias\": 256,\n",
            "  \"input_proj.3.0.weight\": 4718592,\n",
            "  \"input_proj.3.0.bias\": 256,\n",
            "  \"input_proj.3.1.weight\": 256,\n",
            "  \"input_proj.3.1.bias\": 256,\n",
            "  \"backbone.0.body.layer2.0.conv1.weight\": 32768,\n",
            "  \"backbone.0.body.layer2.0.conv2.weight\": 147456,\n",
            "  \"backbone.0.body.layer2.0.conv3.weight\": 65536,\n",
            "  \"backbone.0.body.layer2.0.downsample.0.weight\": 131072,\n",
            "  \"backbone.0.body.layer2.1.conv1.weight\": 65536,\n",
            "  \"backbone.0.body.layer2.1.conv2.weight\": 147456,\n",
            "  \"backbone.0.body.layer2.1.conv3.weight\": 65536,\n",
            "  \"backbone.0.body.layer2.2.conv1.weight\": 65536,\n",
            "  \"backbone.0.body.layer2.2.conv2.weight\": 147456,\n",
            "  \"backbone.0.body.layer2.2.conv3.weight\": 65536,\n",
            "  \"backbone.0.body.layer2.3.conv1.weight\": 65536,\n",
            "  \"backbone.0.body.layer2.3.conv2.weight\": 147456,\n",
            "  \"backbone.0.body.layer2.3.conv3.weight\": 65536,\n",
            "  \"backbone.0.body.layer3.0.conv1.weight\": 131072,\n",
            "  \"backbone.0.body.layer3.0.conv2.weight\": 589824,\n",
            "  \"backbone.0.body.layer3.0.conv3.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.0.downsample.0.weight\": 524288,\n",
            "  \"backbone.0.body.layer3.1.conv1.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.1.conv2.weight\": 589824,\n",
            "  \"backbone.0.body.layer3.1.conv3.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.2.conv1.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.2.conv2.weight\": 589824,\n",
            "  \"backbone.0.body.layer3.2.conv3.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.3.conv1.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.3.conv2.weight\": 589824,\n",
            "  \"backbone.0.body.layer3.3.conv3.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.4.conv1.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.4.conv2.weight\": 589824,\n",
            "  \"backbone.0.body.layer3.4.conv3.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.5.conv1.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.5.conv2.weight\": 589824,\n",
            "  \"backbone.0.body.layer3.5.conv3.weight\": 262144,\n",
            "  \"backbone.0.body.layer4.0.conv1.weight\": 524288,\n",
            "  \"backbone.0.body.layer4.0.conv2.weight\": 2359296,\n",
            "  \"backbone.0.body.layer4.0.conv3.weight\": 1048576,\n",
            "  \"backbone.0.body.layer4.0.downsample.0.weight\": 2097152,\n",
            "  \"backbone.0.body.layer4.1.conv1.weight\": 1048576,\n",
            "  \"backbone.0.body.layer4.1.conv2.weight\": 2359296,\n",
            "  \"backbone.0.body.layer4.1.conv3.weight\": 1048576,\n",
            "  \"backbone.0.body.layer4.2.conv1.weight\": 1048576,\n",
            "  \"backbone.0.body.layer4.2.conv2.weight\": 2359296,\n",
            "  \"backbone.0.body.layer4.2.conv3.weight\": 1048576\n",
            "}\n",
            "data_aug_params: {\n",
            "  \"scales\": [\n",
            "    480,\n",
            "    512,\n",
            "    544,\n",
            "    576,\n",
            "    608,\n",
            "    640,\n",
            "    672,\n",
            "    704,\n",
            "    736,\n",
            "    768,\n",
            "    800\n",
            "  ],\n",
            "  \"max_size\": 1333,\n",
            "  \"scales2_resize\": [\n",
            "    400,\n",
            "    500,\n",
            "    600\n",
            "  ],\n",
            "  \"scales2_crop\": [\n",
            "    384,\n",
            "    600\n",
            "  ]\n",
            "}\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "data_aug_params: {\n",
            "  \"scales\": [\n",
            "    480,\n",
            "    512,\n",
            "    544,\n",
            "    576,\n",
            "    608,\n",
            "    640,\n",
            "    672,\n",
            "    704,\n",
            "    736,\n",
            "    768,\n",
            "    800\n",
            "  ],\n",
            "  \"max_size\": 1333,\n",
            "  \"scales2_resize\": [\n",
            "    400,\n",
            "    500,\n",
            "    600\n",
            "  ],\n",
            "  \"scales2_crop\": [\n",
            "    384,\n",
            "    600\n",
            "  ]\n",
            "}\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/content/DINO/DINO/main.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.resume, map_location='cpu')\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/DINO/DINO/main.py\", line 388, in <module>\n",
            "    main(args)\n",
            "  File \"/content/DINO/DINO/main.py\", line 212, in main\n",
            "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1065, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 468, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 449, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'checkpoint0011_4scale.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1eeAHgu-fzp28PGdIjeLe-pzGPMG2r2G_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIx_8bybNKZD",
        "outputId": "440e61ec-34fd-44e5-c5ba-0c751c268589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1eeAHgu-fzp28PGdIjeLe-pzGPMG2r2G_\n",
            "From (redirected): https://drive.google.com/uc?id=1eeAHgu-fzp28PGdIjeLe-pzGPMG2r2G_&confirm=t&uuid=d953c0c2-5e79-4c42-8557-6e9cce6a9b09\n",
            "To: /content/DINO/DINO/checkpoint0011_4scale.pth\n",
            "100% 562M/562M [00:06<00:00, 81.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinstall requirements after changing NumPy version\n",
        "!pip install -r requirements.txt\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cl06lQTQMrw",
        "outputId": "35ed7d8c-e83f-42a4-dddc-a1d658562597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycocotools (from -r requirements.txt (line 2))\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-install-0th0jaqc/pycocotools_6083d2d95a774e3e941bde4345162bcc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-install-0th0jaqc/pycocotools_6083d2d95a774e3e941bde4345162bcc\n",
            "  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting panopticapi (from -r requirements.txt (line 6))\n",
            "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-install-0th0jaqc/panopticapi_757d5d2b7f264efe9276688100161a54\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/panopticapi.git /tmp/pip-install-0th0jaqc/panopticapi_757d5d2b7f264efe9276688100161a54\n",
            "  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.0.11)\n",
            "Requirement already satisfied: submitit in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.5.2)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.19.1+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.13.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.4.0)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.4.0)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.31.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (1.0.9)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools->-r requirements.txt (line 2)) (71.0.4)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools->-r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 3)) (2.2.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 5)) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 5)) (10.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 11)) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 11)) (0.4.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 11)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 11)) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->-r requirements.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.0->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the file\n",
        "file_path = '/usr/local/lib/python3.10/dist-packages/pycocotools/cocoeval.py'\n",
        "\n",
        "# Open the file, read its contents, and replace np.float with float\n",
        "with open(file_path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Replace np.float with float\n",
        "content = content.replace('np.float', 'float')\n",
        "\n",
        "# Write the modified content back to the file\n",
        "with open(file_path, 'w') as file:\n",
        "    file.write(content)\n",
        "\n",
        "print(\"Replacement completed.\")\n"
      ],
      "metadata": {
        "id": "Pl5aJpguTlFJ",
        "outputId": "c0703d05-5f0e-4327-a7a9-a7b982abde8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replacement completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation\n",
        "!bash scripts/DINO_eval.sh {coco_dir} checkpoint0011_4scale.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iBOhVaRNH2E",
        "outputId": "6e6a9de4-dd0b-462b-ebca-04d9d84c9252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Loading config file from config/DINO/DINO_4scale.py\n",
            "[09/24 09:18:47.455]: git:\n",
            "  sha: d84a491d41898b3befd8294d1cf2614661fc0953, status: clean, branch: main\n",
            "\n",
            "[09/24 09:18:47.455]: Command: main.py --output_dir logs/DINO/R50-MS4-%j -c config/DINO/DINO_4scale.py --coco_path /content/COCODIR --eval --resume checkpoint0011_4scale.pth --options dn_scalar=100 embed_init_tgt=TRUE dn_label_coef=1.0 dn_bbox_coef=1.0 use_ema=False dn_box_noise_scale=1.0\n",
            "[09/24 09:18:47.456]: Full config saved to logs/DINO/R50-MS4-%j/config_args_all.json\n",
            "[09/24 09:18:47.456]: world size: 1\n",
            "[09/24 09:18:47.456]: rank: 0\n",
            "[09/24 09:18:47.456]: local_rank: 0\n",
            "[09/24 09:18:47.456]: args: Namespace(config_file='config/DINO/DINO_4scale.py', options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'use_ema': False, 'dn_box_noise_scale': 1.0}, dataset_file='coco', coco_path='/content/COCODIR', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/DINO/R50-MS4-%j', note='', device='cuda', seed=42, resume='checkpoint0011_4scale.pth', pretrain_model_path=None, finetune_ignore=None, start_epoch=0, eval=True, num_workers=10, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=0, amp=False, distributed=False, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_max_size=1333, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, num_classes=91, lr=0.0001, param_dict_type='default', lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, ddetr_lr_param=False, batch_size=2, weight_decay=0.0001, epochs=12, lr_drop=11, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[33, 45], modelname='dino', frozen_weights=None, backbone='resnet50', use_checkpoint=False, dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, unic_layers=0, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, num_feature_levels=4, enc_n_points=4, dec_n_points=4, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_type='standard', two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, transformer_activation='relu', batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=2.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=1.0, mask_loss_coef=1.0, dice_loss_coef=1.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, focal_alpha=0.25, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, use_dn=True, dn_number=100, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, embed_init_tgt=True, dn_labelbook_size=91, match_unstable_error=True, use_ema=False, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, dn_scalar=100, dn_label_coef=1.0, dn_bbox_coef=1.0)\n",
            "\n",
            "Namespace(config_file='config/DINO/DINO_4scale.py', options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'use_ema': False, 'dn_box_noise_scale': 1.0}, dataset_file='coco', coco_path='/content/COCODIR', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/DINO/R50-MS4-%j', note='', device='cuda', seed=42, resume='checkpoint0011_4scale.pth', pretrain_model_path=None, finetune_ignore=None, start_epoch=0, eval=True, num_workers=10, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=0, amp=False, distributed=False, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_max_size=1333, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, num_classes=91, lr=0.0001, param_dict_type='default', lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, ddetr_lr_param=False, batch_size=2, weight_decay=0.0001, epochs=12, lr_drop=11, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[33, 45], modelname='dino', frozen_weights=None, backbone='resnet50', use_checkpoint=False, dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, unic_layers=0, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, num_feature_levels=4, enc_n_points=4, dec_n_points=4, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_type='standard', two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, transformer_activation='relu', batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=2.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=1.0, mask_loss_coef=1.0, dice_loss_coef=1.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, focal_alpha=0.25, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, use_dn=True, dn_number=100, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, embed_init_tgt=True, dn_labelbook_size=91, match_unstable_error=True, use_ema=False, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, dn_scalar=100, dn_label_coef=1.0, dn_bbox_coef=1.0)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "[09/24 09:18:49.055]: number of params:46670782\n",
            "[09/24 09:18:49.056]: params:\n",
            "{\n",
            "  \"transformer.level_embed\": 1024,\n",
            "  \"transformer.encoder.layers.0.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.0.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.0.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.0.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.0.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.0.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.0.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.0.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.0.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.0.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.0.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.1.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.1.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.1.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.1.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.1.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.1.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.1.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.1.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.1.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.1.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.2.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.2.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.2.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.2.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.2.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.2.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.2.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.2.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.2.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.2.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.3.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.3.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.3.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.3.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.3.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.3.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.3.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.3.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.3.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.3.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.4.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.4.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.4.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.4.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.4.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.4.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.4.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.4.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.4.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.4.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.5.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.5.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.5.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.5.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.5.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.5.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.5.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.5.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.5.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.5.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.0.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.0.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.0.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.0.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.0.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.0.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.0.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.0.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.0.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.0.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.1.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.1.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.1.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.1.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.1.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.1.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.1.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.1.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.1.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.1.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.2.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.2.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.2.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.2.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.2.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.2.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.2.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.2.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.2.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.2.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.3.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.3.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.3.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.3.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.3.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.3.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.3.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.3.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.3.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.3.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.4.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.4.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.4.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.4.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.4.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.4.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.4.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.4.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.4.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.4.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.5.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.5.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.5.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.5.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.5.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.5.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.5.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.5.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.5.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.5.norm3.bias\": 256,\n",
            "  \"transformer.decoder.norm.weight\": 256,\n",
            "  \"transformer.decoder.norm.bias\": 256,\n",
            "  \"transformer.decoder.ref_point_head.layers.0.weight\": 131072,\n",
            "  \"transformer.decoder.ref_point_head.layers.0.bias\": 256,\n",
            "  \"transformer.decoder.ref_point_head.layers.1.weight\": 65536,\n",
            "  \"transformer.decoder.ref_point_head.layers.1.bias\": 256,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.0.weight\": 65536,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.0.bias\": 256,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.1.weight\": 65536,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.1.bias\": 256,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.2.weight\": 1024,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.2.bias\": 4,\n",
            "  \"transformer.decoder.class_embed.0.weight\": 23296,\n",
            "  \"transformer.decoder.class_embed.0.bias\": 91,\n",
            "  \"transformer.tgt_embed.weight\": 230400,\n",
            "  \"transformer.enc_output.weight\": 65536,\n",
            "  \"transformer.enc_output.bias\": 256,\n",
            "  \"transformer.enc_output_norm.weight\": 256,\n",
            "  \"transformer.enc_output_norm.bias\": 256,\n",
            "  \"transformer.enc_out_bbox_embed.layers.0.weight\": 65536,\n",
            "  \"transformer.enc_out_bbox_embed.layers.0.bias\": 256,\n",
            "  \"transformer.enc_out_bbox_embed.layers.1.weight\": 65536,\n",
            "  \"transformer.enc_out_bbox_embed.layers.1.bias\": 256,\n",
            "  \"transformer.enc_out_bbox_embed.layers.2.weight\": 1024,\n",
            "  \"transformer.enc_out_bbox_embed.layers.2.bias\": 4,\n",
            "  \"transformer.enc_out_class_embed.weight\": 23296,\n",
            "  \"transformer.enc_out_class_embed.bias\": 91,\n",
            "  \"label_enc.weight\": 23552,\n",
            "  \"input_proj.0.0.weight\": 131072,\n",
            "  \"input_proj.0.0.bias\": 256,\n",
            "  \"input_proj.0.1.weight\": 256,\n",
            "  \"input_proj.0.1.bias\": 256,\n",
            "  \"input_proj.1.0.weight\": 262144,\n",
            "  \"input_proj.1.0.bias\": 256,\n",
            "  \"input_proj.1.1.weight\": 256,\n",
            "  \"input_proj.1.1.bias\": 256,\n",
            "  \"input_proj.2.0.weight\": 524288,\n",
            "  \"input_proj.2.0.bias\": 256,\n",
            "  \"input_proj.2.1.weight\": 256,\n",
            "  \"input_proj.2.1.bias\": 256,\n",
            "  \"input_proj.3.0.weight\": 4718592,\n",
            "  \"input_proj.3.0.bias\": 256,\n",
            "  \"input_proj.3.1.weight\": 256,\n",
            "  \"input_proj.3.1.bias\": 256,\n",
            "  \"backbone.0.body.layer2.0.conv1.weight\": 32768,\n",
            "  \"backbone.0.body.layer2.0.conv2.weight\": 147456,\n",
            "  \"backbone.0.body.layer2.0.conv3.weight\": 65536,\n",
            "  \"backbone.0.body.layer2.0.downsample.0.weight\": 131072,\n",
            "  \"backbone.0.body.layer2.1.conv1.weight\": 65536,\n",
            "  \"backbone.0.body.layer2.1.conv2.weight\": 147456,\n",
            "  \"backbone.0.body.layer2.1.conv3.weight\": 65536,\n",
            "  \"backbone.0.body.layer2.2.conv1.weight\": 65536,\n",
            "  \"backbone.0.body.layer2.2.conv2.weight\": 147456,\n",
            "  \"backbone.0.body.layer2.2.conv3.weight\": 65536,\n",
            "  \"backbone.0.body.layer2.3.conv1.weight\": 65536,\n",
            "  \"backbone.0.body.layer2.3.conv2.weight\": 147456,\n",
            "  \"backbone.0.body.layer2.3.conv3.weight\": 65536,\n",
            "  \"backbone.0.body.layer3.0.conv1.weight\": 131072,\n",
            "  \"backbone.0.body.layer3.0.conv2.weight\": 589824,\n",
            "  \"backbone.0.body.layer3.0.conv3.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.0.downsample.0.weight\": 524288,\n",
            "  \"backbone.0.body.layer3.1.conv1.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.1.conv2.weight\": 589824,\n",
            "  \"backbone.0.body.layer3.1.conv3.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.2.conv1.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.2.conv2.weight\": 589824,\n",
            "  \"backbone.0.body.layer3.2.conv3.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.3.conv1.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.3.conv2.weight\": 589824,\n",
            "  \"backbone.0.body.layer3.3.conv3.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.4.conv1.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.4.conv2.weight\": 589824,\n",
            "  \"backbone.0.body.layer3.4.conv3.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.5.conv1.weight\": 262144,\n",
            "  \"backbone.0.body.layer3.5.conv2.weight\": 589824,\n",
            "  \"backbone.0.body.layer3.5.conv3.weight\": 262144,\n",
            "  \"backbone.0.body.layer4.0.conv1.weight\": 524288,\n",
            "  \"backbone.0.body.layer4.0.conv2.weight\": 2359296,\n",
            "  \"backbone.0.body.layer4.0.conv3.weight\": 1048576,\n",
            "  \"backbone.0.body.layer4.0.downsample.0.weight\": 2097152,\n",
            "  \"backbone.0.body.layer4.1.conv1.weight\": 1048576,\n",
            "  \"backbone.0.body.layer4.1.conv2.weight\": 2359296,\n",
            "  \"backbone.0.body.layer4.1.conv3.weight\": 1048576,\n",
            "  \"backbone.0.body.layer4.2.conv1.weight\": 1048576,\n",
            "  \"backbone.0.body.layer4.2.conv2.weight\": 2359296,\n",
            "  \"backbone.0.body.layer4.2.conv3.weight\": 1048576\n",
            "}\n",
            "data_aug_params: {\n",
            "  \"scales\": [\n",
            "    480,\n",
            "    512,\n",
            "    544,\n",
            "    576,\n",
            "    608,\n",
            "    640,\n",
            "    672,\n",
            "    704,\n",
            "    736,\n",
            "    768,\n",
            "    800\n",
            "  ],\n",
            "  \"max_size\": 1333,\n",
            "  \"scales2_resize\": [\n",
            "    400,\n",
            "    500,\n",
            "    600\n",
            "  ],\n",
            "  \"scales2_crop\": [\n",
            "    384,\n",
            "    600\n",
            "  ]\n",
            "}\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "data_aug_params: {\n",
            "  \"scales\": [\n",
            "    480,\n",
            "    512,\n",
            "    544,\n",
            "    576,\n",
            "    608,\n",
            "    640,\n",
            "    672,\n",
            "    704,\n",
            "    736,\n",
            "    768,\n",
            "    800\n",
            "  ],\n",
            "  \"max_size\": 1333,\n",
            "  \"scales2_resize\": [\n",
            "    400,\n",
            "    500,\n",
            "    600\n",
            "  ],\n",
            "  \"scales2_crop\": [\n",
            "    384,\n",
            "    600\n",
            "  ]\n",
            "}\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/content/DINO/DINO/main.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.resume, map_location='cpu')\n",
            "/content/DINO/DINO/engine.py:164: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=args.amp):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Test:  [ 0/40]  eta: 0:01:47  class_error: 0.00  loss: 6.3711 (6.3711)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.1946 (0.1946)  loss_bbox: 0.0986 (0.0986)  loss_giou: 0.6058 (0.6058)  loss_ce_0: 0.2246 (0.2246)  loss_bbox_0: 0.0957 (0.0957)  loss_giou_0: 0.5984 (0.5984)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.2025 (0.2025)  loss_bbox_1: 0.1030 (0.1030)  loss_giou_1: 0.6284 (0.6284)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.1964 (0.1964)  loss_bbox_2: 0.0986 (0.0986)  loss_giou_2: 0.6008 (0.6008)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.1926 (0.1926)  loss_bbox_3: 0.0988 (0.0988)  loss_giou_3: 0.6066 (0.6066)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.1960 (0.1960)  loss_bbox_4: 0.0984 (0.0984)  loss_giou_4: 0.6057 (0.6057)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.2061 (0.2061)  loss_bbox_interm: 0.0942 (0.0942)  loss_giou_interm: 0.6254 (0.6254)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.1946 (0.1946)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.0197 (0.0197)  loss_giou_unscaled: 0.3029 (0.3029)  loss_xy_unscaled: 0.0062 (0.0062)  loss_hw_unscaled: 0.0135 (0.0135)  cardinality_error_unscaled: 877.0000 (877.0000)  loss_ce_0_unscaled: 0.2246 (0.2246)  loss_bbox_0_unscaled: 0.0191 (0.0191)  loss_giou_0_unscaled: 0.2992 (0.2992)  loss_xy_0_unscaled: 0.0060 (0.0060)  loss_hw_0_unscaled: 0.0131 (0.0131)  cardinality_error_0_unscaled: 877.0000 (877.0000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.2025 (0.2025)  loss_bbox_1_unscaled: 0.0206 (0.0206)  loss_giou_1_unscaled: 0.3142 (0.3142)  loss_xy_1_unscaled: 0.0067 (0.0067)  loss_hw_1_unscaled: 0.0139 (0.0139)  cardinality_error_1_unscaled: 877.0000 (877.0000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.1964 (0.1964)  loss_bbox_2_unscaled: 0.0197 (0.0197)  loss_giou_2_unscaled: 0.3004 (0.3004)  loss_xy_2_unscaled: 0.0062 (0.0062)  loss_hw_2_unscaled: 0.0135 (0.0135)  cardinality_error_2_unscaled: 877.0000 (877.0000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.1926 (0.1926)  loss_bbox_3_unscaled: 0.0198 (0.0198)  loss_giou_3_unscaled: 0.3033 (0.3033)  loss_xy_3_unscaled: 0.0063 (0.0063)  loss_hw_3_unscaled: 0.0135 (0.0135)  cardinality_error_3_unscaled: 877.0000 (877.0000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.1960 (0.1960)  loss_bbox_4_unscaled: 0.0197 (0.0197)  loss_giou_4_unscaled: 0.3028 (0.3028)  loss_xy_4_unscaled: 0.0062 (0.0062)  loss_hw_4_unscaled: 0.0134 (0.0134)  cardinality_error_4_unscaled: 877.0000 (877.0000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.2061 (0.2061)  loss_bbox_interm_unscaled: 0.0188 (0.0188)  loss_giou_interm_unscaled: 0.3127 (0.3127)  loss_xy_interm_unscaled: 0.0058 (0.0058)  loss_hw_interm_unscaled: 0.0130 (0.0130)  cardinality_error_interm_unscaled: 877.0000 (877.0000)  time: 2.6766  data: 0.5955  max mem: 647\n",
            "Test:  [10/40]  eta: 0:00:14  class_error: 0.00  loss: 6.1351 (6.2312)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.1876 (0.2322)  loss_bbox: 0.0775 (0.0924)  loss_giou: 0.5587 (0.5788)  loss_ce_0: 0.2028 (0.2125)  loss_bbox_0: 0.0774 (0.0929)  loss_giou_0: 0.5734 (0.5682)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.1964 (0.2187)  loss_bbox_1: 0.0760 (0.0935)  loss_giou_1: 0.5622 (0.5737)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.1934 (0.2109)  loss_bbox_2: 0.0768 (0.0931)  loss_giou_2: 0.5549 (0.5781)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.1791 (0.2114)  loss_bbox_3: 0.0774 (0.0956)  loss_giou_3: 0.6066 (0.5859)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.1830 (0.2221)  loss_bbox_4: 0.0776 (0.0955)  loss_giou_4: 0.6057 (0.5861)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.2413 (0.2233)  loss_bbox_interm: 0.0834 (0.0931)  loss_giou_interm: 0.5681 (0.5730)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.1876 (0.2322)  class_error_unscaled: 0.0000 (1.2987)  loss_bbox_unscaled: 0.0155 (0.0185)  loss_giou_unscaled: 0.2793 (0.2894)  loss_xy_unscaled: 0.0052 (0.0060)  loss_hw_unscaled: 0.0103 (0.0124)  cardinality_error_unscaled: 892.0000 (888.4545)  loss_ce_0_unscaled: 0.2028 (0.2125)  loss_bbox_0_unscaled: 0.0155 (0.0186)  loss_giou_0_unscaled: 0.2867 (0.2841)  loss_xy_0_unscaled: 0.0052 (0.0060)  loss_hw_0_unscaled: 0.0103 (0.0126)  cardinality_error_0_unscaled: 892.0000 (888.4545)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.1964 (0.2187)  loss_bbox_1_unscaled: 0.0152 (0.0187)  loss_giou_1_unscaled: 0.2811 (0.2869)  loss_xy_1_unscaled: 0.0052 (0.0062)  loss_hw_1_unscaled: 0.0100 (0.0125)  cardinality_error_1_unscaled: 892.0000 (888.4545)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.1934 (0.2109)  loss_bbox_2_unscaled: 0.0154 (0.0186)  loss_giou_2_unscaled: 0.2775 (0.2890)  loss_xy_2_unscaled: 0.0051 (0.0061)  loss_hw_2_unscaled: 0.0103 (0.0125)  cardinality_error_2_unscaled: 892.0000 (888.4545)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.1791 (0.2114)  loss_bbox_3_unscaled: 0.0155 (0.0191)  loss_giou_3_unscaled: 0.3033 (0.2929)  loss_xy_3_unscaled: 0.0052 (0.0061)  loss_hw_3_unscaled: 0.0103 (0.0130)  cardinality_error_3_unscaled: 892.0000 (888.4545)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.1830 (0.2221)  loss_bbox_4_unscaled: 0.0155 (0.0191)  loss_giou_4_unscaled: 0.3028 (0.2931)  loss_xy_4_unscaled: 0.0052 (0.0061)  loss_hw_4_unscaled: 0.0103 (0.0130)  cardinality_error_4_unscaled: 892.0000 (888.4545)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.2413 (0.2233)  loss_bbox_interm_unscaled: 0.0167 (0.0186)  loss_giou_interm_unscaled: 0.2840 (0.2865)  loss_xy_interm_unscaled: 0.0055 (0.0060)  loss_hw_interm_unscaled: 0.0112 (0.0126)  cardinality_error_interm_unscaled: 892.0000 (888.4545)  time: 0.4967  data: 0.0671  max mem: 649\n",
            "Test:  [20/40]  eta: 0:00:07  class_error: 0.00  loss: 4.8102 (5.3146)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.1876 (0.2701)  loss_bbox: 0.0610 (0.0777)  loss_giou: 0.4122 (0.4373)  loss_ce_0: 0.2028 (0.2246)  loss_bbox_0: 0.0577 (0.0779)  loss_giou_0: 0.3894 (0.4331)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.1964 (0.2373)  loss_bbox_1: 0.0599 (0.0786)  loss_giou_1: 0.4015 (0.4371)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.1934 (0.2353)  loss_bbox_2: 0.0603 (0.0784)  loss_giou_2: 0.4122 (0.4378)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.1791 (0.2435)  loss_bbox_3: 0.0610 (0.0795)  loss_giou_3: 0.4029 (0.4406)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.1830 (0.2575)  loss_bbox_4: 0.0611 (0.0794)  loss_giou_4: 0.4122 (0.4410)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.2329 (0.2253)  loss_bbox_interm: 0.0593 (0.0804)  loss_giou_interm: 0.3875 (0.4423)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.1876 (0.2701)  class_error_unscaled: 0.0000 (0.6803)  loss_bbox_unscaled: 0.0122 (0.0155)  loss_giou_unscaled: 0.2061 (0.2186)  loss_xy_unscaled: 0.0046 (0.0053)  loss_hw_unscaled: 0.0077 (0.0102)  cardinality_error_unscaled: 894.0000 (892.0952)  loss_ce_0_unscaled: 0.2028 (0.2246)  loss_bbox_0_unscaled: 0.0115 (0.0156)  loss_giou_0_unscaled: 0.1947 (0.2165)  loss_xy_0_unscaled: 0.0040 (0.0053)  loss_hw_0_unscaled: 0.0071 (0.0103)  cardinality_error_0_unscaled: 894.0000 (892.0952)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.1964 (0.2373)  loss_bbox_1_unscaled: 0.0120 (0.0157)  loss_giou_1_unscaled: 0.2007 (0.2186)  loss_xy_1_unscaled: 0.0044 (0.0054)  loss_hw_1_unscaled: 0.0076 (0.0103)  cardinality_error_1_unscaled: 894.0000 (892.0952)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.1934 (0.2353)  loss_bbox_2_unscaled: 0.0121 (0.0157)  loss_giou_2_unscaled: 0.2061 (0.2189)  loss_xy_2_unscaled: 0.0045 (0.0054)  loss_hw_2_unscaled: 0.0078 (0.0103)  cardinality_error_2_unscaled: 894.0000 (892.0952)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.1791 (0.2435)  loss_bbox_3_unscaled: 0.0122 (0.0159)  loss_giou_3_unscaled: 0.2014 (0.2203)  loss_xy_3_unscaled: 0.0046 (0.0053)  loss_hw_3_unscaled: 0.0078 (0.0105)  cardinality_error_3_unscaled: 894.0000 (892.0952)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.1830 (0.2575)  loss_bbox_4_unscaled: 0.0122 (0.0159)  loss_giou_4_unscaled: 0.2061 (0.2205)  loss_xy_4_unscaled: 0.0046 (0.0053)  loss_hw_4_unscaled: 0.0077 (0.0105)  cardinality_error_4_unscaled: 894.0000 (892.0952)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.2329 (0.2253)  loss_bbox_interm_unscaled: 0.0119 (0.0161)  loss_giou_interm_unscaled: 0.1938 (0.2212)  loss_xy_interm_unscaled: 0.0042 (0.0053)  loss_hw_interm_unscaled: 0.0074 (0.0108)  cardinality_error_interm_unscaled: 894.0000 (892.0952)  time: 0.2677  data: 0.0138  max mem: 649\n",
            "Test:  [30/40]  eta: 0:00:03  class_error: 0.00  loss: 4.8801 (5.6317)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.1989 (0.2596)  loss_bbox: 0.0777 (0.0997)  loss_giou: 0.3830 (0.4694)  loss_ce_0: 0.2003 (0.2257)  loss_bbox_0: 0.0751 (0.0968)  loss_giou_0: 0.3829 (0.4586)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.2013 (0.2376)  loss_bbox_1: 0.0768 (0.0981)  loss_giou_1: 0.3772 (0.4655)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.2049 (0.2304)  loss_bbox_2: 0.0789 (0.0993)  loss_giou_2: 0.3770 (0.4690)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.1923 (0.2370)  loss_bbox_3: 0.0776 (0.1005)  loss_giou_3: 0.3775 (0.4714)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.2037 (0.2503)  loss_bbox_4: 0.0778 (0.0995)  loss_giou_4: 0.3837 (0.4694)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.1916 (0.2250)  loss_bbox_interm: 0.0807 (0.0996)  loss_giou_interm: 0.3780 (0.4693)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.1989 (0.2596)  class_error_unscaled: 0.0000 (0.9217)  loss_bbox_unscaled: 0.0155 (0.0199)  loss_giou_unscaled: 0.1915 (0.2347)  loss_xy_unscaled: 0.0059 (0.0069)  loss_hw_unscaled: 0.0090 (0.0130)  cardinality_error_unscaled: 894.0000 (892.4839)  loss_ce_0_unscaled: 0.2003 (0.2257)  loss_bbox_0_unscaled: 0.0150 (0.0194)  loss_giou_0_unscaled: 0.1915 (0.2293)  loss_xy_0_unscaled: 0.0049 (0.0067)  loss_hw_0_unscaled: 0.0090 (0.0126)  cardinality_error_0_unscaled: 894.0000 (892.4839)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.2013 (0.2376)  loss_bbox_1_unscaled: 0.0154 (0.0196)  loss_giou_1_unscaled: 0.1886 (0.2328)  loss_xy_1_unscaled: 0.0059 (0.0068)  loss_hw_1_unscaled: 0.0090 (0.0128)  cardinality_error_1_unscaled: 894.0000 (892.4839)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.2049 (0.2304)  loss_bbox_2_unscaled: 0.0158 (0.0199)  loss_giou_2_unscaled: 0.1885 (0.2345)  loss_xy_2_unscaled: 0.0060 (0.0069)  loss_hw_2_unscaled: 0.0091 (0.0130)  cardinality_error_2_unscaled: 894.0000 (892.4839)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.1923 (0.2370)  loss_bbox_3_unscaled: 0.0155 (0.0201)  loss_giou_3_unscaled: 0.1888 (0.2357)  loss_xy_3_unscaled: 0.0059 (0.0069)  loss_hw_3_unscaled: 0.0089 (0.0132)  cardinality_error_3_unscaled: 894.0000 (892.4839)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.2037 (0.2503)  loss_bbox_4_unscaled: 0.0156 (0.0199)  loss_giou_4_unscaled: 0.1918 (0.2347)  loss_xy_4_unscaled: 0.0059 (0.0069)  loss_hw_4_unscaled: 0.0090 (0.0130)  cardinality_error_4_unscaled: 894.0000 (892.4839)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.1916 (0.2250)  loss_bbox_interm_unscaled: 0.0161 (0.0199)  loss_giou_interm_unscaled: 0.1890 (0.2347)  loss_xy_interm_unscaled: 0.0054 (0.0069)  loss_hw_interm_unscaled: 0.0094 (0.0131)  cardinality_error_interm_unscaled: 894.0000 (892.4839)  time: 0.2422  data: 0.0104  max mem: 649\n",
            "Test:  [39/40]  eta: 0:00:00  class_error: 0.00  loss: 5.5146 (5.7767)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.1861 (0.2749)  loss_bbox: 0.0615 (0.1047)  loss_giou: 0.4758 (0.4669)  loss_ce_0: 0.1988 (0.2404)  loss_bbox_0: 0.0656 (0.1038)  loss_giou_0: 0.4358 (0.4636)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.2004 (0.2527)  loss_bbox_1: 0.0624 (0.1047)  loss_giou_1: 0.4312 (0.4693)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.1907 (0.2458)  loss_bbox_2: 0.0620 (0.1045)  loss_giou_2: 0.4753 (0.4662)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.1914 (0.2543)  loss_bbox_3: 0.0620 (0.1054)  loss_giou_3: 0.4698 (0.4679)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.1822 (0.2674)  loss_bbox_4: 0.0616 (0.1046)  loss_giou_4: 0.4759 (0.4671)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.1999 (0.2380)  loss_bbox_interm: 0.0685 (0.1049)  loss_giou_interm: 0.4634 (0.4694)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.1861 (0.2749)  class_error_unscaled: 0.0000 (0.7143)  loss_bbox_unscaled: 0.0123 (0.0209)  loss_giou_unscaled: 0.2379 (0.2335)  loss_xy_unscaled: 0.0042 (0.0071)  loss_hw_unscaled: 0.0085 (0.0138)  cardinality_error_unscaled: 893.0000 (892.5000)  loss_ce_0_unscaled: 0.1988 (0.2404)  loss_bbox_0_unscaled: 0.0131 (0.0208)  loss_giou_0_unscaled: 0.2179 (0.2318)  loss_xy_0_unscaled: 0.0045 (0.0070)  loss_hw_0_unscaled: 0.0084 (0.0138)  cardinality_error_0_unscaled: 893.0000 (892.5000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.2004 (0.2527)  loss_bbox_1_unscaled: 0.0125 (0.0209)  loss_giou_1_unscaled: 0.2156 (0.2346)  loss_xy_1_unscaled: 0.0040 (0.0070)  loss_hw_1_unscaled: 0.0085 (0.0139)  cardinality_error_1_unscaled: 893.0000 (892.5000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.1907 (0.2458)  loss_bbox_2_unscaled: 0.0124 (0.0209)  loss_giou_2_unscaled: 0.2376 (0.2331)  loss_xy_2_unscaled: 0.0042 (0.0071)  loss_hw_2_unscaled: 0.0085 (0.0138)  cardinality_error_2_unscaled: 893.0000 (892.5000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.1914 (0.2543)  loss_bbox_3_unscaled: 0.0124 (0.0211)  loss_giou_3_unscaled: 0.2349 (0.2339)  loss_xy_3_unscaled: 0.0041 (0.0071)  loss_hw_3_unscaled: 0.0085 (0.0140)  cardinality_error_3_unscaled: 893.0000 (892.5000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.1822 (0.2674)  loss_bbox_4_unscaled: 0.0123 (0.0209)  loss_giou_4_unscaled: 0.2380 (0.2336)  loss_xy_4_unscaled: 0.0042 (0.0071)  loss_hw_4_unscaled: 0.0085 (0.0138)  cardinality_error_4_unscaled: 893.0000 (892.5000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.1999 (0.2380)  loss_bbox_interm_unscaled: 0.0137 (0.0210)  loss_giou_interm_unscaled: 0.2317 (0.2347)  loss_xy_interm_unscaled: 0.0046 (0.0070)  loss_hw_interm_unscaled: 0.0087 (0.0140)  cardinality_error_interm_unscaled: 893.0000 (892.5000)  time: 0.2270  data: 0.0071  max mem: 649\n",
            "Test: Total time: 0:00:12 (0.3129 s / it)\n",
            "Averaged stats: class_error: 0.00  loss: 5.5146 (5.7767)  loss_bbox_dn: 0.0000 (0.0000)  loss_giou_dn: 0.0000 (0.0000)  loss_ce_dn: 0.0000 (0.0000)  loss_ce: 0.1861 (0.2749)  loss_bbox: 0.0615 (0.1047)  loss_giou: 0.4758 (0.4669)  loss_ce_0: 0.1988 (0.2404)  loss_bbox_0: 0.0656 (0.1038)  loss_giou_0: 0.4358 (0.4636)  loss_bbox_dn_0: 0.0000 (0.0000)  loss_giou_dn_0: 0.0000 (0.0000)  loss_ce_dn_0: 0.0000 (0.0000)  loss_ce_1: 0.2004 (0.2527)  loss_bbox_1: 0.0624 (0.1047)  loss_giou_1: 0.4312 (0.4693)  loss_bbox_dn_1: 0.0000 (0.0000)  loss_giou_dn_1: 0.0000 (0.0000)  loss_ce_dn_1: 0.0000 (0.0000)  loss_ce_2: 0.1907 (0.2458)  loss_bbox_2: 0.0620 (0.1045)  loss_giou_2: 0.4753 (0.4662)  loss_bbox_dn_2: 0.0000 (0.0000)  loss_giou_dn_2: 0.0000 (0.0000)  loss_ce_dn_2: 0.0000 (0.0000)  loss_ce_3: 0.1914 (0.2543)  loss_bbox_3: 0.0620 (0.1054)  loss_giou_3: 0.4698 (0.4679)  loss_bbox_dn_3: 0.0000 (0.0000)  loss_giou_dn_3: 0.0000 (0.0000)  loss_ce_dn_3: 0.0000 (0.0000)  loss_ce_4: 0.1822 (0.2674)  loss_bbox_4: 0.0616 (0.1046)  loss_giou_4: 0.4759 (0.4671)  loss_bbox_dn_4: 0.0000 (0.0000)  loss_giou_dn_4: 0.0000 (0.0000)  loss_ce_dn_4: 0.0000 (0.0000)  loss_ce_interm: 0.1999 (0.2380)  loss_bbox_interm: 0.0685 (0.1049)  loss_giou_interm: 0.4634 (0.4694)  loss_bbox_dn_unscaled: 0.0000 (0.0000)  loss_giou_dn_unscaled: 0.0000 (0.0000)  loss_ce_dn_unscaled: 0.0000 (0.0000)  loss_xy_dn_unscaled: 0.0000 (0.0000)  loss_hw_dn_unscaled: 0.0000 (0.0000)  cardinality_error_dn_unscaled: 0.0000 (0.0000)  loss_ce_unscaled: 0.1861 (0.2749)  class_error_unscaled: 0.0000 (0.7143)  loss_bbox_unscaled: 0.0123 (0.0209)  loss_giou_unscaled: 0.2379 (0.2335)  loss_xy_unscaled: 0.0042 (0.0071)  loss_hw_unscaled: 0.0085 (0.0138)  cardinality_error_unscaled: 893.0000 (892.5000)  loss_ce_0_unscaled: 0.1988 (0.2404)  loss_bbox_0_unscaled: 0.0131 (0.0208)  loss_giou_0_unscaled: 0.2179 (0.2318)  loss_xy_0_unscaled: 0.0045 (0.0070)  loss_hw_0_unscaled: 0.0084 (0.0138)  cardinality_error_0_unscaled: 893.0000 (892.5000)  loss_bbox_dn_0_unscaled: 0.0000 (0.0000)  loss_giou_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_dn_0_unscaled: 0.0000 (0.0000)  loss_xy_dn_0_unscaled: 0.0000 (0.0000)  loss_hw_dn_0_unscaled: 0.0000 (0.0000)  cardinality_error_dn_0_unscaled: 0.0000 (0.0000)  loss_ce_1_unscaled: 0.2004 (0.2527)  loss_bbox_1_unscaled: 0.0125 (0.0209)  loss_giou_1_unscaled: 0.2156 (0.2346)  loss_xy_1_unscaled: 0.0040 (0.0070)  loss_hw_1_unscaled: 0.0085 (0.0139)  cardinality_error_1_unscaled: 893.0000 (892.5000)  loss_bbox_dn_1_unscaled: 0.0000 (0.0000)  loss_giou_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_dn_1_unscaled: 0.0000 (0.0000)  loss_xy_dn_1_unscaled: 0.0000 (0.0000)  loss_hw_dn_1_unscaled: 0.0000 (0.0000)  cardinality_error_dn_1_unscaled: 0.0000 (0.0000)  loss_ce_2_unscaled: 0.1907 (0.2458)  loss_bbox_2_unscaled: 0.0124 (0.0209)  loss_giou_2_unscaled: 0.2376 (0.2331)  loss_xy_2_unscaled: 0.0042 (0.0071)  loss_hw_2_unscaled: 0.0085 (0.0138)  cardinality_error_2_unscaled: 893.0000 (892.5000)  loss_bbox_dn_2_unscaled: 0.0000 (0.0000)  loss_giou_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_dn_2_unscaled: 0.0000 (0.0000)  loss_xy_dn_2_unscaled: 0.0000 (0.0000)  loss_hw_dn_2_unscaled: 0.0000 (0.0000)  cardinality_error_dn_2_unscaled: 0.0000 (0.0000)  loss_ce_3_unscaled: 0.1914 (0.2543)  loss_bbox_3_unscaled: 0.0124 (0.0211)  loss_giou_3_unscaled: 0.2349 (0.2339)  loss_xy_3_unscaled: 0.0041 (0.0071)  loss_hw_3_unscaled: 0.0085 (0.0140)  cardinality_error_3_unscaled: 893.0000 (892.5000)  loss_bbox_dn_3_unscaled: 0.0000 (0.0000)  loss_giou_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_dn_3_unscaled: 0.0000 (0.0000)  loss_xy_dn_3_unscaled: 0.0000 (0.0000)  loss_hw_dn_3_unscaled: 0.0000 (0.0000)  cardinality_error_dn_3_unscaled: 0.0000 (0.0000)  loss_ce_4_unscaled: 0.1822 (0.2674)  loss_bbox_4_unscaled: 0.0123 (0.0209)  loss_giou_4_unscaled: 0.2380 (0.2336)  loss_xy_4_unscaled: 0.0042 (0.0071)  loss_hw_4_unscaled: 0.0085 (0.0138)  cardinality_error_4_unscaled: 893.0000 (892.5000)  loss_bbox_dn_4_unscaled: 0.0000 (0.0000)  loss_giou_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_dn_4_unscaled: 0.0000 (0.0000)  loss_xy_dn_4_unscaled: 0.0000 (0.0000)  loss_hw_dn_4_unscaled: 0.0000 (0.0000)  cardinality_error_dn_4_unscaled: 0.0000 (0.0000)  loss_ce_interm_unscaled: 0.1999 (0.2380)  loss_bbox_interm_unscaled: 0.0137 (0.0210)  loss_giou_interm_unscaled: 0.2317 (0.2347)  loss_xy_interm_unscaled: 0.0046 (0.0070)  loss_hw_interm_unscaled: 0.0087 (0.0140)  cardinality_error_interm_unscaled: 893.0000 (892.5000)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.03s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.502\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.854\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.541\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.412\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.593\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.714\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.104\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.507\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.621\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.568\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.680\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify config file\n",
        "!cp config/DINO/DINO_4scale.py config/DINO/DINO_4scale_r50_iitd.py\n",
        "!sed -i 's/coco_path = \"path\\/to\\/your\\/dataset\"/coco_path = \"\\/content\\/COCODIR\"/' config/DINO/DINO_4scale_r50_iitd.py\n",
        "!sed -i 's/max_epoch = 12/max_epoch = 5/' config/DINO/DINO_4scale_r50_iitd.py\n",
        "!sed -i 's/lr = 0.0001/lr = 0.00001/' config/DINO/DINO_4scale_r50_iitd.py\n",
        "\n",
        "# Run evaluation on pre-trained model\n",
        "!python main.py --output_dir ./output_pretrained_eval --with_box_refine --two_stage --num_classes 2 --coco_path {coco_dir} --batch_size 2 --resume checkpoint0011_4scale.pth --eval --config_file config/DINO/DINO_4scale_r50_iitd.py\n",
        "\n",
        "# Fine-tune the model\n",
        "!python main.py --output_dir ./output_iitd --with_box_refine --two_stage --num_classes 2 --epochs 5 --lr_drop 4 --coco_path {coco_dir} --batch_size 2 --resume checkpoint0011_4scale.pth --config_file config/DINO/DINO_4scale_r50_iitd.py\n",
        "\n",
        "# Evaluate fine-tuned model\n",
        "!python main.py --output_dir ./output_iitd_eval --with_box_refine --two_stage --num_classes 2 --coco_path {coco_dir} --batch_size 2 --resume output_iitd/checkpoint.pth --eval --config_file config/DINO/DINO_4scale_r50_iitd.py\n"
      ],
      "metadata": {
        "id": "gBDazCvH79qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from util.slconfig import SLConfig\n",
        "from models.dino.dino import build_dino\n",
        "from util.visualizer import COCOVisualizer\n",
        "def visualize_results():\n",
        "    args = SLConfig.fromfile(\"config/DINO/DINO_4scale_r50_iitd.py\")\n",
        "    args.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    model = build_dino(args)\n",
        "    checkpoint = torch.load(\"output_iitd/checkpoint.pth\", map_location=\"cpu\")\n",
        "    model.load_state_dict(checkpoint[\"model\"])\n",
        "    model.to(args.device)\n",
        "    model.eval()\n",
        "\n",
        "    image_path = \"/content/COCODIR/val2017/image1.jpg\"  # Replace with an actual image path\n",
        "    image = Image.open(image_path)\n",
        "    transform = T.Compose([\n",
        "        T.Resize(800),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    image_tensor = transform(image).unsqueeze(0).to(args.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "\n",
        "    visualizer = COCOVisualizer()\n",
        "    result = visualizer.visualize(image, outputs, threshold=0.5)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(result)\n",
        "    plt.axis('off')\n",
        "    plt.savefig('/content/drive/MyDrive/detection_result.png')\n",
        "    plt.close()\n",
        "\n",
        "visualize_results()\n",
        "print(\"Visualization saved to Google Drive as 'detection_result.png'\")\n",
        "\n",
        "# Generate loss graph\n",
        "def generate_loss_graph():\n",
        "    with open('output_iitd/log.txt', 'r') as f:\n",
        "        log = f.readlines()\n",
        "\n",
        "    epochs = []\n",
        "    losses = []\n",
        "\n",
        "    for line in log:\n",
        "        if 'epoch: ' in line and 'loss: ' in line:\n",
        "            data = json.loads(line.split('INFO: ')[1])\n",
        "            epochs.append(data['epoch'])\n",
        "            losses.append(data['loss'])\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, losses)\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.savefig('/content/drive/MyDrive/training_loss.png')\n",
        "    plt.close()\n",
        "\n",
        "generate_loss_graph()\n",
        "print(\"Training loss graph saved to Google Drive as 'training_loss.png'\")\n",
        "\n",
        "# Save fine-tuned model weights to Google Drive\n",
        "!cp output_iitd/checkpoint.pth /content/drive/MyDrive/dino_finetuned_iitd.pth\n"
      ],
      "metadata": {
        "id": "m0f9uleL7a1M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}